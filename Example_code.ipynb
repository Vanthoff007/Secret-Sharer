{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Set Environment ","metadata":{}},{"cell_type":"code","source":"# Install the 'Secret Sharer' package from GitHub and other requirements\nfrom IPython.display import clear_output\n\n# !pip install transformers datasets matplotlib numpy torch accelerate\n!pip install git+https://github.com/Vanthoff007/SecretSharer.git\n\nclear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:13:46.591760Z","iopub.execute_input":"2024-11-06T07:13:46.592386Z","iopub.status.idle":"2024-11-06T07:14:02.335039Z","shell.execute_reply.started":"2024-11-06T07:13:46.592348Z","shell.execute_reply":"2024-11-06T07:14:02.333866Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 2. Generate Canaries","metadata":{}},{"cell_type":"code","source":"# Step 1: Import and Define Configuration for Canary Generation\nfrom Generate_Canaries import CanaryDatasetGenerator\n\n# Step 2: Define Pattern and Vocabulary for Canaries\n# The 'patterns' variable is a list containing a pattern for canary generation.\n# The placeholders (e.g., {}{}{}{}) will be filled with random values from 'vocabs' during generation.\npatterns = ['My name is Pikachu Pandey and my social security code is: {}{}']\n\n# The 'vocabs' variable specifies the vocabulary used for the placeholders in 'patterns'.\n# Here, '0123456789' is used to generate a 4-digit code for each canary.\nvocabs = [list('0123456789')]  # Digits for filling placeholders in pattern\n\n# Step 3: Specify Repetition and Quantity of Canaries\n# 'num_repetitions' defines the number of times to repeat the pattern.\n# 'num_secrets_for_repetition' defines the number of secrets generated for each repetition in 'num_repetitions'.\nnum_repetitions = [1]  # Generate each canary pattern only once\nnum_secrets_for_repetition = [1] * len(num_repetitions)  # One secret per pattern repetition\n\n# Calculate the number of references for each canary configuration\n# 'num_references' is the number of additional non-canary references generated for each pattern.\n# Itâ€™s set to be a large number (e.g., 10,000) minus the number of generated canaries.\nnum_references = 10**2 - sum(num_secrets_for_repetition)\n\n# Step 4: Create Configuration Dictionary for Canary Generation\n# We use a list comprehension to build a dictionary of configurations.\n# Each configuration contains the pattern, vocabulary, repetition, and counts for canary generation.\nsecret_configs = [\n    {\n        'vocabulary': vocab,                     # Vocabulary for filling placeholders\n        'pattern': pattern,                      # Pattern template for canary\n        'repetitions': num_repetitions,          # Number of times to repeat the pattern\n        'secrets_per_repetition': num_secrets_for_repetition,  # Canaries per repetition\n        'num_references': num_references         # Additional reference texts\n    }\n    for vocab, pattern in zip(vocabs, patterns)  # Pair each vocab with its corresponding pattern\n]\n\n# Step 5: Generate Canaries and References with CanaryDatasetGenerator\n# 'Datasets' will store the generated datasets for each configuration.\nDatasets = []\n\n# Loop through each configuration in 'secret_configs' and create canary datasets\nfor config in secret_configs:\n    # Initialize the generator with configuration parameters\n    generator = CanaryDatasetGenerator(\n        vocabulary=config['vocabulary'],                # Vocabulary for canary generation\n        pattern=config['pattern'],                      # Pattern with placeholders\n        repetitions=config['repetitions'],              # Number of repetitions for pattern\n        secrets_per_repetition=config['secrets_per_repetition'],  # Quantity of secrets\n        num_references=config['num_references'],        # Reference text count\n        seed=0  # Set random seed for reproducibility\n    )\n    \n    # Generate dataset containing canaries and references\n    result = generator.create_dataset()\n    \n    # Append generated dataset to the Datasets list\n    Datasets.append(result)\n    \n    # Display sample output to verify correctness\n    print(f\"Generated dataset for pattern '{config['pattern']}'\")\n    print(\"Dataset Sample:\", result['dataset'][:5])  # Display first 5 canary entries\n    print(\"References Sample:\", result['references'][:5])  # Display first 5 reference entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:02.337062Z","iopub.execute_input":"2024-11-06T07:14:02.337392Z","iopub.status.idle":"2024-11-06T07:14:02.360064Z","shell.execute_reply.started":"2024-11-06T07:14:02.337355Z","shell.execute_reply":"2024-11-06T07:14:02.359085Z"}},"outputs":[{"name":"stdout","text":"Generated dataset for pattern 'My name is Pikachu Pandey and my social security code is: {}{}'\nDataset Sample: ['My name is Pikachu Pandey and my social security code is: 78']\nReferences Sample: ['My name is Pikachu Pandey and my social security code is: 54', 'My name is Pikachu Pandey and my social security code is: 94', 'My name is Pikachu Pandey and my social security code is: 67', 'My name is Pikachu Pandey and my social security code is: 87', 'My name is Pikachu Pandey and my social security code is: 20']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 3. Prepare Dataset\n\nThis class processes a Hugging Face dataset to prepare it for training, specifically targeting datasets with text structures formatted by newlines.","metadata":{}},{"cell_type":"code","source":"class PrepareData:\n    def __init__(self, dataset_name, generated_canaries=None, max_length=1024, batch_size=8, tokenizer=None):\n        \"\"\"\n        Initialize the PrepareData class with dataset, tokenizer, and processing parameters.\n\n        Args:\n            dataset_name (str): Name of the dataset from Hugging Face.\n            generated_canaries (list, optional): List of additional data to append to train set.\n            max_length (int): Maximum token length for each sequence.\n            batch_size (int): Number of samples per batch.\n            tokenizer_name (str): Hugging Face tokenizer model to use.\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.generated_canaries = generated_canaries or []\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.dataset = load_dataset(dataset_name, trust_remote_code = True)\n        self.tokenizer = tokenizer\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n    def _process_text_split(self, split_data):\n        \"\"\"\n        Processes text split by replacing newline characters and splitting into manageable lengths.\n\n        Args:\n            split_data (list): List of text lines from a dataset split.\n\n        Returns:\n            list: List of processed and split text lines.\n        \"\"\"\n        processed = [line.replace(\"\\\\n\", \" \") for line in split_data.split(\"\\\\n\\\\n\")]\n        return self._split_long_lines(processed)\n\n    def _split_long_lines(self, lines):\n        \"\"\"\n        Splits text into smaller chunks based on max_length.\n\n        Args:\n            lines (list): List of text lines.\n\n        Returns:\n            list: List of text chunks respecting max_length.\n        \"\"\"\n        split_lines = []\n        for line in lines:\n            while len(line) > self.max_length:\n                split_index = line.rfind(\" \", 0, self.max_length) or self.max_length\n                split_lines.append(line[:split_index].strip())\n                line = line[split_index:].strip()\n            split_lines.append(line)\n        return split_lines\n\n    def prepare_dataset(self):\n        \"\"\"\n        Prepares the dataset splits (train, validation, test), adds generated canaries, and processes text.\n\n        Returns:\n            tuple: Processed train, validation, and test data.\n        \"\"\"\n        train_data = self._process_text_split(self.dataset['train']['text'][0])\n        validation_data = self._process_text_split(self.dataset['validation']['text'][0])\n        test_data = self._process_text_split(self.dataset['test']['text'][0])\n\n        # Append canaries to training data if provided\n        for results in self.generated_canaries:\n           for canaries in results[\"dataset\"]:\n               train_data.append(canaries)\n\n        return train_data, validation_data, test_data\n\n    def create_dataloader(self, dataset, shuffle=True):\n        \"\"\"\n        Creates a PyTorch DataLoader with tokenized data.\n\n        Args:\n            dataset (list): List of text data for tokenization.\n            shuffle (bool): If True, shuffles data; else processes sequentially.\n\n        Returns:\n            DataLoader: DataLoader for PyTorch batch processing.\n        \"\"\"\n        if not self.tokenizer:\n            raise ValueError(\"Pass a Tokenizer\")\n        \n        # Tokenize dataset in batch for efficiency\n        encodings = self.tokenizer(dataset, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n        encodes = list(zip(encodings[\"input_ids\"], encodings[\"attention_mask\"]))\n\n        # Define sampler and DataLoader\n        sampler = RandomSampler(encodes) if shuffle else SequentialSampler(encodes)\n        return DataLoader(encodes, sampler=sampler, batch_size=self.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:04.882734Z","iopub.execute_input":"2024-11-06T07:14:04.883119Z","iopub.status.idle":"2024-11-06T07:14:04.898615Z","shell.execute_reply.started":"2024-11-06T07:14:04.883083Z","shell.execute_reply":"2024-11-06T07:14:04.897731Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 4. Train Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW, AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nimport random\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:05.926445Z","iopub.execute_input":"2024-11-06T07:14:05.927206Z","iopub.status.idle":"2024-11-06T07:14:25.680393Z","shell.execute_reply.started":"2024-11-06T07:14:05.927169Z","shell.execute_reply":"2024-11-06T07:14:25.679572Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Important Parameters\nMAX_LENGTH = 1024\nBATCH_SIZE = 4\n\nEPOCHS = 3\nLEARNING_RATE = 5e-5\nWARMUP_STEPS = 1e2\nEPSILON = 1e-8\n\ndataset_name = \"tiny_shakespeare\"\nmodel_name = \"distilbert/distilgpt2\"\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\naccelerator = Accelerator()\ndevice = accelerator.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:25.682102Z","iopub.execute_input":"2024-11-06T07:14:25.682676Z","iopub.status.idle":"2024-11-06T07:14:25.751223Z","shell.execute_reply.started":"2024-11-06T07:14:25.682640Z","shell.execute_reply":"2024-11-06T07:14:25.750218Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, tokenizer, train_dataloader, validation_dataloader, epochs, learning_rate, epsilon, warmup_steps, device):\n    \"\"\"\n    Trains a language model using the specified dataloaders, optimizer, and scheduler.\n\n    Args:\n        model: The language model to train.\n        train_dataloader: DataLoader for training data.\n        validation_dataloader: DataLoader for validation data.\n        epochs: Number of training epochs.\n        learning_rate: Learning rate for model training.\n        epsilon: Epsilon value for optimiser\n        warmup_steps: Warmup steps for scheduler.\n        sample_every: Interval for generating samples during training.\n        tokenizer: Tokenizer for decoding generated text samples.\n        device: Device (CPU or GPU) for model training.\n\n    Returns:\n        training_stats: List of dictionaries containing training and validation loss per epoch.\n    \"\"\"\n    training_stats = []\n    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n    \n    # Total number of training steps is [number of batches] x [number of epochs].\n    total_steps = len(train_dataloader) * EPOCHS\n    \n    # This changes the learning rate as the training loop progresses\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps)\n    model.resize_token_embeddings(len(tokenizer))\n    model = model.to(device)\n    \n    for epoch_i in range(0, epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n        print('Training...')\n\n        total_train_loss = 0\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            batch_input_ids = batch[0].to(device)\n            batch_labels = batch[0].to(device)\n            batch_masks = batch[1].to(device)\n\n            model.zero_grad()\n\n            outputs = model(batch_input_ids, labels=batch_labels, attention_mask=batch_masks, token_type_ids=None)\n            loss = outputs[0]\n            batch_loss = loss.item()\n            total_train_loss += batch_loss\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        # Calculate average training loss\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"Average training loss: {avg_train_loss:.2f}\", end = \"\\n\")\n\n        # Validation phase\n        print(\"\\nRunning Validation...\")\n        model.eval()\n\n        total_eval_loss = 0\n\n        for batch in validation_dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_labels = batch[0].to(device)\n            batch_masks = batch[1].to(device)\n            \n            with torch.no_grad():\n                outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n                loss = outputs[0]\n\n            batch_loss = loss.item()\n            total_eval_loss += batch_loss\n\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n\n        # Record statistics for this epoch\n        training_stats.append({\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss\n        })\n\n    print(\"\\nTraining complete!\")\n    return training_stats, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:25.752686Z","iopub.execute_input":"2024-11-06T07:14:25.753045Z","iopub.status.idle":"2024-11-06T07:14:25.767285Z","shell.execute_reply.started":"2024-11-06T07:14:25.753011Z","shell.execute_reply":"2024-11-06T07:14:25.766076Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code = True)\n\n# Instantiate the class\nprepare_data = PrepareData(\n    dataset_name=dataset_name,\n    generated_canaries=Datasets,\n    max_length=MAX_LENGTH,\n    batch_size=BATCH_SIZE,\n    tokenizer=tokenizer\n)\n\n# Prepare the dataset: Get train, validation, and test splits\ntrain_data, validation_data, test_data = prepare_data.prepare_dataset()\n\n# Example: Create DataLoader for training data\ntrain_loader = prepare_data.create_dataloader(train_data[:8], shuffle=True)\nval_loader = prepare_data.create_dataloader(validation_data[:8], shuffle=False)\n\n# Loop through the DataLoader to check batches\nfor batch in train_loader:\n    input_ids, attention_masks = batch\n    print(\"Batch of input IDs:\", input_ids)\n    print(\"Batch of attention masks:\", attention_masks)\n    break  # Only display the first batch for demonstration\n\ntraining_stats, model = train_model(\n    model=model, \n    tokenizer=tokenizer, \n    train_dataloader=train_loader, \n    validation_dataloader=val_loader, \n    epochs=1, \n    learning_rate=LEARNING_RATE, \n    epsilon=EPSILON, \n    warmup_steps=WARMUP_STEPS, \n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:25.769035Z","iopub.execute_input":"2024-11-06T07:14:25.769337Z","iopub.status.idle":"2024-11-06T07:14:33.645310Z","shell.execute_reply.started":"2024-11-06T07:14:25.769303Z","shell.execute_reply":"2024-11-06T07:14:33.644330Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3dd2b6d2b5544a29971102633a4bfdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27853d776e344a878871d64dc75f17aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d62d176bd4412db2d3223f702d0f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9586aa8a69e24128bb494035ef7284fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36233759b2b74ee0882842892e9467dc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed7be6dc50c436b95a033f4b88d5e58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb15222d01643a396575dec5c6a3fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tiny_shakespeare.py:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5655e887da408188dfac6854ee9222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171d1a564def4ea5a4c9d05d8ff0284f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6b478a69fbb47e9b18c0df074979ca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c57b3e6f0fe942ba96a9903ce72d98e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad781aad14b4dfe98216caace998585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3144f0e71440cfae0e39b90c990c6f"}},"metadata":{}},{"name":"stdout","text":"Batch of input IDs: tensor([[ 2435,   618,   477,  ..., 50257, 50257, 50257],\n        [ 5962, 22307,    25,  ..., 50257, 50257, 50257],\n        [ 1462,  1592,   617,  ..., 50257, 50257, 50257],\n        [ 5832,  5120,  2592,  ..., 50257, 50257, 50257]])\nBatch of attention masks: tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 1 ========\nTraining...\nAverage training loss: 13.36\n\nRunning Validation...\n  Validation Loss: 15.63\n\nTraining complete!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 5. Compute Perplexity","metadata":{}},{"cell_type":"code","source":"# Step 1: Import PerplexityCalculator\nfrom Compute_Perplexity import PerplexityCalculator as PC\n\n# Initialize PerplexityCalculator with specified parameters\n# Arguments:\n#   model: The pre-trained language model for which we calculate perplexity (e.g., GPT-2)\n#   tokenizer: Tokenizer for encoding input text\n#   MAX_LENGTH: Maximum token length for each sequence\n#   device: Device to perform computation (e.g., 'cuda' for GPU, 'cpu' otherwise)\nCalculate_perplexity = PC(model, tokenizer, MAX_LENGTH, device)\n\n# Step 2: Initialize Lists to Store Perplexity Results\nCP, RP = [], []\n\n# Step 3: Loop through Each Generated Dataset to Calculate Perplexities\nfor dataset in Datasets:\n    # Extract Unique Canaries and Reference Texts\n    canary = list(set(dataset[\"dataset\"]))  # List of unique canaries (secrets) from dataset\n    reference = dataset[\"references\"]       # List of reference texts for comparison\n\n    # Step 4: Compute Perplexities for Canaries and Reference Texts\n    \n    # Arguments:\n    #   canary: List of canary texts for which to compute perplexities\n    #   reference: List of reference texts for perplexity comparison \n    # Returns:\n    #   canary_perplexities: Perplexity values for each canary\n    #   reference_perplexities: Perplexity values for each reference text\n    canary_perplexities, reference_perplexities = Calculate_perplexity.compute_perplexities_for_canaries(canary, reference)\n    \n    # Step 5: Append Results to Lists\n    CP.append(canary_perplexities)  # Append canary perplexities for current dataset\n    RP.append(reference_perplexities)  # Append reference perplexities for current dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:36.373731Z","iopub.execute_input":"2024-11-06T07:14:36.374125Z","iopub.status.idle":"2024-11-06T07:14:39.593401Z","shell.execute_reply.started":"2024-11-06T07:14:36.374088Z","shell.execute_reply":"2024-11-06T07:14:39.592567Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"CP, RP[0][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:15:14.921889Z","iopub.execute_input":"2024-11-06T07:15:14.922702Z","iopub.status.idle":"2024-11-06T07:15:14.929926Z","shell.execute_reply.started":"2024-11-06T07:15:14.922648Z","shell.execute_reply":"2024-11-06T07:15:14.928765Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"([{'My name is Pikachu Pandey and my social security code is: 78': 1.852993369102478}],\n [1.8521897792816162,\n  1.8509958982467651,\n  1.8519335985183716,\n  1.8517086505889893,\n  1.8490588665008545])"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# 6. Compute Exposure","metadata":{}},{"cell_type":"code","source":"# Step 1: Import ComputeExposure Class\nfrom Compute_Exposure import ComputeExposure\n\n# Step 2: Initialize an Empty List to Store Exposure Results\nexposures = []\n\n# Step 3: Loop Through Each Set of Canary and Reference Perplexities\n# CP and RP were previously defined lists where:\n#   - CP[i] contains the perplexity values of canaries for the i-th dataset\n#   - RP[i] contains the perplexity values of reference texts for the i-th dataset\nfor i in range(len(CP)):\n    # Step 4: Compute Exposure Score Using ComputeExposure\n    # Arguments:\n    #   CP[i]: Perplexity values for the canaries of the i-th dataset\n    #   RP[i]: Perplexity values for the references of the i-th dataset\n    # Returns:\n    #   exp: Exposure score calculated using the 'rank method'\n    exp = ComputeExposure(CP[i], RP[i]).compute_exposure_rank_method()\n    \n    # Append the exposure score for the current dataset to the exposures list\n    exposures.append(exp)\n\n# Step 5: Print Exposure Results for All Datasets\nprint(exposures)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T07:14:44.668312Z","iopub.execute_input":"2024-11-06T07:14:44.668874Z","iopub.status.idle":"2024-11-06T07:14:44.678666Z","shell.execute_reply.started":"2024-11-06T07:14:44.668718Z","shell.execute_reply":"2024-11-06T07:14:44.677264Z"}},"outputs":[{"name":"stdout","text":"[{'My name is Pikachu Pandey and my social security code is: 78': 0.07400058144377653}]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}